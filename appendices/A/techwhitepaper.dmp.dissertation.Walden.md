APPENDIX A: TECHNICAL WHITEPAPER AND DATA MANAGEMENT PLAN
Data Sources, Forms, and Acquisition
The core dataset for this project includes both biographical and transactional information about the players who constitute U.S. professional baseball’s workforce in the post-World War II era. While that core data is available publicly at Baseball Reference’s website (run by Sport Reference, LLC), licensed by Hidden Game, its public-facing presence does not present the data in ways that facilitate or make possible the type of queries and research questions undertaken in this project. As shown in Figure 110, information about player birthplaces and dates does not exist publicly in tabular form, which prevents an analysis of the aggregate player biographical information, since the data about player birthplaces is not connected with the data about their playing careers and transactions. Additionally, the playing transaction information included on a specific player’s page does not include enough descriptive information about the teams that player was a member of, like the classification level and team location, to support in-depth analysis of aggregate data trends (see Figures 112-117). Other site pages include roster information about a specific professional team in a specific season and do connect player birthplaces with the record of a specific team’s roster (see Figures 116-117). However, the team-level data is isolated on season-specific pages, making it difficult to analyze change over time across teams and seasons. Additionally, the team-level data does not include additional relevant information like the team location and classification level, preventing or inhibiting an analysis of large-scale aggregate trends and change over time. While the data that supports this project is publicly available, it is not publicly available or accessible in a way/form/structure that allows the type of large-scale aggregate analysis undertaken in this project. 
Initially, data collection was done manually, copying and pasting online data into Excel and reformatting to enable cross-table links resembling a database structure (see Figures 118-119). Additionally, initial efforts were made by the University of Iowa’s Digital Studio for Scholarship and Publishing toward developing a web crawling script that would automate the one data scraping, given the potential for error and the amount of labor and time necessary to manually collect the online data. However, a closer inspection of Sports Reference LLC’s terms of use raised several questions about the legality of manually gathering the data or using an automated scraping method to compile the data. After further inspection of Sports Reference LLC’s terms and conditions (see Appendix B), no further attempt was made to automate data gathering or gather the data manually until Baseball Reference, Sports Reference LLC, and Hidden Game Sports (third party data licenser and host) had been contacted to clarify the terms of use and gain permission to use the data for this project. 
Given the fragmented, unstructured, and unconnected way much of that data lives online, collecting/gathering it manually would have been a time and labor-intensive process, delaying the overall progress of the project. Additionally, while scraping the data from the website would have addressed some of those time and labor concerns, doing so was a violation of the website’s terms of service. And, since the facets of the data central to this research project were not uniformly structured, some degree of manual cleaning and data wrangling would be necessary to structure the data in a uniform way. The original goal of scraping data was to collect player biographical information, team transaction information, and team affiliation and location information. Again, given the labor and time necessary to gather the data manually, the Digital Studio (and I) were interested in procuring a bespoke dataset customized to include the data points related to this project’s research questions.
Original efforts to gather the data manually focused on the aspects of facets of the data needed to respond to the project’s research questions, specifically player places and dates of birth, teams played for, team classifications and affiliations, and team locations. Initial efforts to gather this data manually utilized the data structure outlined in Figures 118 and 119. However, once the decision was made to move away from manual or automated data collection toward a bespoke licensed dataset, this original structure was abandoned, given the limited feasibility of significantly restructuring the licensed dataset. After reaching a licensing agreement with Hidden Game, the licensed dataset was delivered via email as a zipped folder with CSV three CSV files—Playing, Locations, and Affiliations. The data included in those three CSV files included the elements outlined in Appendix D. In the original data formation, there were multiple data redundancies, first with player biographical information being duplicated in the “Playing” table to correlate with multiple transaction records for specific players. Additionally, the “Locations” table includes location information for Major and Minor League Teams, but other relevant locations—specifically player birthplaces—are located elsewhere in the table data structure. A crucial needed piece of information—a unique identifiers for teams—is not included in the original dataset, and the affiliation level information is included in “Playing” table. See Figure 120 for a relational schema (RS) for the original data, and Appendix D for sample data structures and field descriptions. 
Immediately, several choices were made to exclude certain data points deemed not statistically significant or of central relevance to the project’s research questions. Specifically, the player ethnicity, death date, and death location information were removed, given the lack of completeness and statistical significance for the data provided in this field. Ethnicity information was not provided or available for 70,486 players in the dataset, which represents 87.1% of the players in the dataset. Additionally, upon further conversations with the party who oversaw the curation and development of the dataset, ethnicity data was determined either by looking at a player’s place of birth or phenotypical presentation (skin color). Given this dissertation’s commitment to a cultural studies and American Studies orientation that frames identity as fluid, constructed, contingent, and inherently personal, a decision was made to remove the “Ethnicity” data field from future calculations, both due to the limited nuance in the methodology used to determine ethnicity as well as the lack of substantive data representation in this field.
Additionally, death date information was not provided or available for 78,226 distinct players in the dataset, which represents 96.6% of the players in the dataset, and death location information was not provided or available for 78,735 distinct players in the dataset, which represents 97.2% of the players in the dataset. While player lifespan and place of death would be an interesting, relevant, and related area of research for further study, the lack of substantive data in those fields within this dataset prompted the decision to not use those fields in the data analysis. Player ethnicity, death date, and death location information were removed, and the number of games played at specific positions was removed from the functional dataset, yielding the RS diagram in Figure 121.
In the original data formation, there were multiple data redundancies, first with player biographical information being duplicated in the “Playing” table to correlate with multiple transaction records for specific players. Additionally, the “Locations” table includes location information for Major and Minor League Teams, but other relevant locations—specifically player birthplaces—are located elsewhere in the table data structure. A crucial needed piece of information—a unique identifiers for teams—is not included in the original dataset, and the affiliation level information is included in “Playing” table. Additionally, since the size and complexity of the “Playing” table was exacerbated by the duplicated player biographical information, a decision was made to separate the original “Playing” table into “Biographical” and “Transactional” tables that consolidated player biographical information and removed the duplicated biographical information from the transactional records. 
While redundancies in the player biographical information had been removed, information about teams, affiliations, classifications, and team locations was not logically organized within the data-table structure. While the licensed data as originally delivered included the data points needed for the research questions undertaken in this project, the data did not exist in a structure or organized form that allowed or made possible the type of inquiry central to this project. While the primary motivation for many of these manipulations or restructurings was an attempt to wrangle the data into a structure that could be imported into a database, or at least allow for coherent cross-table connections, it is important to remember that manipulations of data or reconfiguring of data structure is predicated on or necessitates a position of authority over the data and what it represents. Determining what fields and data points are and are not significant is not a neutral or impersonal act—decisions about structuring data reflect a broader notion of knowledge and power in relation to what data (especially complex quantitative data) represents and communicates.  In these data manipulations, the goal of these manipulations was to preserve the fields that were statistically significant and central or useful for the central research questions driving the larger project. 
To more fully understand the significant aspects of the data and their relationship, an entity-relationship diagram (see Figure 122) was made to help clarify what might work as a more logical table structure. See Appendix D for labels and definitions for the significant aspects of the data to help simplifies the data structure and Major League affiliations, I am using MLB’s own abbreviations to describe the thirty MLB teams. This nomenclature constitutes a standardized or controlled vocabulary within the professional baseball and baseball research communities. However, a small number of Major League teams represented in the dataset relocated or changed names before evolving into or being replaced by the current list of professional teams. That nomenclature is outlined in Appendix F.
Having defined or articulated the key elements of the dataset to link and organize in tabular form, several significant restructuring decisions were made. In addition to separating player biographical and transactional information in separate tables (a change represented in the last relational diagram), the complexity of team attributes (location, classification, and affiliation information) suggested the utility of consolidating that information for all Major and Minor League teams in a single affiliation table. That change is reflected in the RS presented in Figure 123. While that change to the data structure helped consolidate the team attributes, the need to have three foreign keys in the affiliation table to match on the transactional table involved data redundancy that could be streamlined through a unique team identifier that replaced the three matching fields currently in the transactional table. Making that revision to the data structure yielded the RS outlined in Figure 124.
The only major remaining redundancy or illogical dimension to the data structure was the location information. In this iteration of the relational structure, location information existed in two tables—the “Biographical” table that included player birthplace locations, and the “Locations” table that included team location information. Since all locations would need to be geocoded for spatial analysis, a decision was made to create a master location table and create unique location identifiers that would connect to the “Biographical” and “Affiliation” tables. 
Georeferencing the data was accomplished using GPSVisualizer and a MapQuest API.  The advantage of using the automated API tool for georeferencing was the limited amount of manual labor required to georeferenced the over 11,000 locations represented in the dataset. GPSVisualizer’s capacity to batch code large numbers of global addresses relatively quickly with minimal hands-on oversight required. However, that automation and limited oversight does increase the potential for errors in the georeferencing, which when noticed require manual correction. The ease of automated georeferencing belies a more complex and nuanced discussion of digital labor and the (often uncompensated) labor required to create or generate effective automated tools like GPSVisaulizer.  Using an automated georeferencing system also raises questions about where scholarly labor is situated or who is responsible for the labor needed to accomplish the various components of a digital project.
That shift yielded the (final) data structure represented in Figure 125. Some redundancies were kept in the data structure to facilitate faster queries and easier visualizations, specifically the birth_country field in the “Biographical” table and the classification field in the “Transactional” table. Additionally, given the significant number of players and birth countries represented, a decision was made to additionally code country locations by region to facilitate more meaningful macro-level analysis of location trends, specifically for player birthplaces. The United Nations’ “Standard Country or Area Codes for Statistical Use,” Series M, No. 49, was used to organize countries into regions and groups according to the UN taxonomy, which is provided in Appendix F.  With the region and group information added to the “Locations” table, the final data schema adheres to the structure outlined in Figure 1215 (RS) and Figure 126 (entity relationship diagram). Sample data tables can be found in Appendix D.
Additional Section 1 data:
While the dataset described previously is the foundation for the overall dissertation project, additional datasets have been gathered, harvested, or procured from other sources to supplement the foundational dataset. The first content section of the dissertation focuses on animating and making visible labor structures around U.S. professional baseball labor, with a focus on Minor League Baseball and the structures and forces that govern its conditions of labor. In addition to information about the numbers of players and teams, and the change over time in those numbers, this section of the dissertation also addresses representation in the labor structures and organizations that impact or shape professional baseball conditions of labor. While the history of the MLBPA has been written elsewhere and is discussed more fully in the first content section, another focus of the argument in this the composition of the Player Relations Committee that negotiates collective bargaining agreements and other labor negotiation terms and agreements. While this body has been discussed in scholarly literature and some degree of mainstream press coverage, no in-depth analysis has focused on the composition of this committee in relation to the larger pool of U.S. professional baseball players.
Portions of the data analysis and visualization in this section look at the members of the PRC, their roles on the Committee, and for the Player Representatives, their biographical and transactional histories—a similar kind of analysis undertaken of the larger pool of players using the foundational dataset. The data for this analysis is taken from the Collective Bargaining Agreement documents, that list the members of the Player Relations Committee, along with the other bodies and organizations involved in negotiating the Agreement and bound by its terms. Represented in tabular form, the PRC includes the fields defined in Appendix D.
Section 3: social media analysis
As mentioned in the project introduction, the third content section of the project examines questions of cultural production and representation in the digital content and communicates created by and for communities and users affiliated with or connected to U.S. professional baseball. This section of the dissertation accomplishes those goals through a time-bounded social media analysis of the Tweets and other digital content produced or generated by official MLB or MiLB-branded Twitter accounts. While the earlier content sections of the dissertation look at player biographical and transactional data, the focus in this third content section is questions of ideology, meaning, commodity, and cultural production, which required moving beyond the biographical and transactional data to include or incorporate additional data sources and forms of analysis. While the analysis undertaken in the first two content sections lays bare demographic shifts and globalization within baseball labor structures, this third section builds on that foundation to look at the branded content professional teams produce about themselves to explore to what degree the shifting dimensions of professional baseball labor are being considered in the messages teams communicate or present about themselves.
While a sustained analysis of the social media content generated by professional baseball players at the Major and Minor League levels would be of great relevance to the questions explored in this section, the challenge of gathering a comprehensive list of Twitter ids for thousands of professional baseball players presents several logistical and practical challenges. Additionally, the question of player privacy and agency raises serious questions about the ethnics of using personal social media content to address questions of identity, power, and representation. While professional baseball teams are themselves public entities that do not and cannot have a reasonable expectation of privacy or confidentiality in their social media activity, the liminal space which many Minor League players occupy raises significant questions about the degree to which they are public figures or celebrities who are not guaranteed or afforded the same level of privacy given to private individuals. However, Major and Minor League Baseball’s use of player-generated social media content to promote their own social media identity, content, and networks raises several significant, relevant questions about the labor of digital production and structures of labor and compensation for labor that takes place in a digital space.
The decision to uses Twitter data as a representative of the larger social media ecosystem and content generated by professional teams was largely based on the accessibility of Twitter data and the ability to store Twitter data in stable preservation formats via json and csv files. While a number of curation, search, and visualization tools exist for digital media platforms, other social media platforms offer limited access to back-end metadata or the ability to collect digital content in a stable preservation format. However, while the textual content and metadata information for Tweets is accessible, digital preservation questions persist around the ability to collect and preserve multimedia objects contained within Tweets. Additionally, the frequent uses of emojis within Tweets raises questions about text-encoding, representation, and translatability, and collecting Tweet content does not necessarily guarantee ongoing access to internet content linked within a Tweet. 
The first step in approach gathering Twitter data was determining or collecting a list of Twitter accounts associated with professional baseball teams, leagues, and organizations. The list of Twitter ids and accounts is included in Appendix H. The twarc “command line tool (and Python library) for archiving Twitter JSON” developed by Ed Summers and the Maryland Institute for Technology in the Humanities was used to execute Python scripts (excerpted in Appendix H) to gather timeline data for the previously-listed accounts.  These scripts were executed twice weekly from March through November 2018, to capture a relatively thorough snapshot of Twitter content produced by entities affiliated with professional baseball over the course of the regular baseball season. The output of those Twarc scripts was JSON files (sample output excerpted in Appendix H) including the data elements in the string, defined in Appendix E. A complete list of Twitter accounts harvested is presented in Appendix H.
Platform Choices
Section 1 platform and tool choices:
To backtrack somewhat, in addition to the data sources for content sections 1 and 2, several additional choices were made about data cleaning and management tools, as well as analysis and visualization tools and programs. Initially, the data was interacted with via Microsoft Excel, a spreadsheet program, and saved as an Excel workbook and individual CSV (comma-separated value) sheets. That process of exploring the data revealed the amount of duplication and overlap that was addressed through the data restructuring outlined in the previous section. Following the philosophy of minimal computing and minimalism in data structure and organization, consolidating, reorganizing, and streamlining the data made it possible to more effectively make connects across tables, describe various facets of the data, and undertake the analysis and visualization central to the research project. While the data was mostly clean, in the sense that it was largely structured and consistent, the need to generate unique cross-table IDs that were consistent and human-readable required a more hands-on grappling with some of the inconsistencies in the data. For example, players born in the same location may have varying levels of specificity provided for their birth place. While one player might have the city, state/province, and country provided, another player born in the same city might only have the city and country information. Additionally, while team locations were provided in the original “Locations” table, descriptive information about affiliations and classifications was provided in the “Affiliations” and “Playing” tables. A more logical structure for that data was to include descriptive information about teams all in one table, and isolate location information in a master locations table with the georeferenced latitude and longitude.
Through this process, some data was intentionally removed from the dataset (on a field level, as outlined previously), a small number of errors in spelling of city locations were also corrected and standardized. Additionally, in its original form, team names were described by either a city or team name, yielding duplicates within and across specific seasons that were addressed by generating unique team identifiers. Somewhat similarly, Major League teams that occupy the same city had duplicate team name information, and franchises that remained the same franchises across Major League team name change were not connected in the dataset. For example, Chicago was used as the team name for both the Chicago Cubs and the Chicago White Sox, and New York was used for both the New York Yankees and New York Mets. As an example of the other data limitation, the Los Angeles Angels of Anaheim franchise were represented by a range of names in the team name and parent team fields. Cleaning the data involved converting original Major League team names to the controlled team vocabulary listed earlier. 
Although the project in its final form did not utilize a back-end database for the data, the goal in this stage of the project was to standardize and organize the data to a degree that it could be effectively imported into a database. Once the data was more fully understood and the amount of data cleaning and restructuring needed to make the data usable for analysis and visualization, other options for data cleaning and wrangling were explored using Open Refine, before going back to Excel to use both automated and manual operations/functions to standardize/clean and reorganize/restructure the data. Once the data had gone through the restructuring and reformatting operations outlined in previous sections, preliminary analysis was begun using Microsoft Excel’s charts and Pivot Tables functionality to analyze data within specific tables and analyze data through making connections across tables. While the original plan for the data had involved using a back-end database to drive front-end visualizations, initial attempts to migrate the data into MySQL and PostgreSQL ran into frequent and numerous join errors. Additionally, the labor of coding SQL queries to join tables and output specific datasets and the learning curve required to effectively write and execute the SQL queries made working in Excel a more feasible, viable option, although the back-end data base would have been a more stable hosting and preservation solution for the larger dataset and limited the capacity for error and data loss in the calculations that yielded the CSVs for front-end visualizations. Additionally, the decision to use generated CSVs to drive front-end visualizations and interactives was driven by the terms of the licensing agreement, which stipulated that the dataset in full would not be made publicly available. Although stringent digital privacy and information security protocols would have likely addressed the concerns around data privacy and the licensing agreement, visualizing from created CSVs limited or restricted the possibility of someone accessing or acquiring the full back-end dataset.
Since the goal of the data analysis and visualization in this section was interactive visualizations, Microsoft Excel and the Pivot Tables functionality would be limited for generating the desired interactive visualizations. Additionally, Microsoft Excel offered limited functionality for quickly moving between types of visualizations to explore different models for visualizing various facets or aspects of the data. To move toward generating more complex interactive visualizations, R and RStudio with additional packages Plotly and ggplot. However, after working more with R and encountering problems with completing appropriate joins across tables, the decision was made to move away from R as a platform for generating CSVs and visualizations, since those efforts had been working in Excel and Pivot Tables, and the additional advanced statistical calculations provided by R were not required for the project. Had more advanced statistical analysis been necessary for the project, R would have been a suitable platform, but the relatively basic operations of counts and averages did not necessitate R’s advanced statistical capabilities. 
To again explore what type of visualizations might be possible or illuminating or useful with the data and imagine what type of interactivity or labeling might be possible or desirable, a combination of Tableau Public and Tableau Professional was used to explore more complex visualizations of the dataset, while also imagining what type of mathematical operations would be desirable to conduct on the dataset. Additionally, Tableau functionality was used to explore preliminary mapping questions and explorations before making a significant investment in a mapping or visualization technology. While Tableau was useful for more rapidly moving toward potential and desired visualizations, the limitations of Tableau Public in terms of handling large, complex datasets and pushing those visualizations to the web (and hosting them on the web via the Tableau public server) made Tableau Public not a feasible long-term platform choice for visualizing or hosting the data or visualizations. Additionally, the project’s commitment to using stable preservation formats and open-source or transparent code and platforms whenever possible prompted a shift away from Tableau as a platform.
That said, the data availability and interactivity provided by Tableau was a goal and the testing or experimental use of Tableau highlighted the desired functionality of visualization. From a preservation perspective, a proprietary platform like Tableau, particularly with the limited functionality of Tableau Public, also does not output stable format that would conform to best practices for digital preservation and long-term storage. Additionally, concerns about the transparency of data visualization and access to back-end data, as well as the cost of long-term use of Tableau as a hosting platform, led to a movement away from an “out of the box” tool like Tableau, toward something based on open-source systems, specifically open-source tools that would yield stable output files types that could be used for long-term digital preservation for the project.
One step toward this goal was the discussion of using an SQL database or back-end database to link content across CSV sheets and drive or push data for front-end visualizations. However, attempts to migrate the data to an SQL database encountered problems with key relationships and creating connections across various tables and fields. Since linking the data in Microsoft Excel via Pivot Tables functionality had been successful in the past and did not produce the same kind of key relationship errors that were continually appearing in the SQL database migration project, the decision was made to keep the back-end data in CSV files, use Pivot Table functionality to output discrete CSV files that would then be used to drive or push to front-end visualizations.
Section 2: mapping + spatial analysis 
As noted in the project introduction and overview, the second content section of the dissertation looks at questions of mapping, spatial analysis, and visualization to explore questions of labor, movement, and place. Given the complexity of the dataset and desired spatial analysis, a robust mapping tool was needed to adequately represent or communicate nuances in the data analysis. Several mapping tools were explored to see what platform, software, or tool might be best situated to execute the spatial analysis dimensions of the project. Initially, because the project originated in an Archives and Media Library Science class, the structure of that class governed the tools that were considered and explored. That class was initially structured around using Palladio as a mapping and visualization tool.  However, even with the dataset in a limited and preliminary state, the complexity in the data’s structure and the need to form or generate additional connections across tables and fields, combined with Palladio’s in-browser loading and operations, caused repeated, significant, and frequent problems and complications when attempting to use Palladio to analyze and visualize the underlying dataset. Additionally, Palladio raised several concerns about generating interactive web visualizations that could be exported and underlying data and code in stable formats for long-term preservation.
Given the challenges found in using Palladio, other platforms were explored for spatial analysis and visualization, starting with Google Fusion Table’s mapping and analysis functions, which quickly were found to be too limited in their strength, customizability, and flexibility to adequately address the needs and priorities of the project. At this point in the project’s timeline, Carto and CartoDB offered free web-hosted versions that also included a range of robust data analysis and customizable interactive visualization tools.  The pilot maps for the project were accomplished in Carto, which provided a model for what type of spatial analysis and visualization would be useful for the project. However, additional exploration of Carto as a platform/tool was halted as other means of accessing, acquiring, and procuring the data were explored. By the time the larger dataset had been procured via a licensing agreement, Carto had shifted its business model to no longer offer a free or relatively affordable site-hosted option for complex spatial data analysis and visualization.
Given the multiple dimensions of the dataset that the project was interested in exploring, analyzing, and visualizing using spatial analysis, the functionality and capacity of ArcGIS quickly became the most useful platform/program for creating an underlying database structure for the dataset, while also generating rich interactive visualizations for the digital platform. Since ArcGIS is a proprietary platform and does not provide open-source back-end code, the preservation solution for these components of the larger project will involve saving stable format, archival quality images of the map visualizations, while also storing as CSV files the back-end data used to generate the visualizations, as well as when available the scripts or queries performed on the back-end data to generate the visualizations used in the project.
Deposit, Preservation, Access 
Given the infrastructure needed to successfully execute a large-scale digital project, accomplishing this project without significant collaborative, administrative, and logistical support would not have been possible. The acquisition of the underlying dataset was facilitated by individuals at Baseball Reference, Sports Reference LLC, and Hidden Game Sports. The University of Iowa’s Digital Studio for Scholarship and Publishing facilitated the licensing agreement and purchase of the data, including a review of the licensing agreement, which is provided in Appendix C.
Ongoing consultation and advisory support for the digital and data driven components of the project were provided by members of the Studio for Digital Scholarship and Publishing staff, in addition to committee members who are DH specialists. Specifically, Nikki White provided consultation and advisory support for the data analysis, structuring, and visualization undertaken in the first content section of the dissertation. GIS specialist Rob Shepard provided advisory and consultation support for the second content section of the dissertation, which focused on mapping and spatial analysis. In addition to providing consulting and advisory support for mapping platform choices and data structures to facilitate spatial analysis and effective mapping. The Digital Humanities Summer Institute’s Geographical Information Systems in the Digital Humanities course taught by Ian Gregory (Lancaster University) was taken in the summer of 2018 to facilitate skill acquisition with the ArcGIS platform. Advanced knowledge of database management, data structuring, data visualization, and digital preservation was acquired through enrollment in Public Digital Humanities Certificate courses, as well as courses that were part of the Library and Information Science degree program.
After the proposal for this dissertation had been approved by an interdisciplinary committee and the Department of American Studies graduate handbook requirements for the dissertation had been altered to allow for an alternative dissertation project, University of Iowa Graduate College academic affairs staff were consulted to articulate what deposit and preservation procedures would look like for an alternative digital dissertation. Specifically, Sarah Larsen (Senior Associate Dean for Academic Affairs and Student Development) was consulted as the American Studies Department moved through the handbook language revision process. Assistant Dean of Academic Affairs Heidi Arbisi-Kelm and Academic Affairs Coordinator Erin Kaufman were key administrative partners in determining what thesis and deposit procedures would for the digital components of the project. Additionally, Tom Keegan, Head of the Digital Scholarship and Publishing Studio, Matthew Butler, Senior Developer of Media Production and Design at the Studio, and Paul Soderdahl, Associate University Librarian at the University of Iowa were also part of the conversation around long-term digital preservation for the project. Procedures for copyright, fair use, licensing, and access were determined in consultation with University of Iowa Counsel’s Office and members of the University of Iowa Libraries’ Scholarly Impact Team, Sarah Scheib and Mahrya Burnett.
The collaborative, workflow, and preservation components of the project were modeled after or shaped by documentation produced by several other large-scale digital projects or guidelines for digital work. Specifically, Amanda Visconti’s documentation for her “Infinite Ulysses” digital dissertation project (University of Maryland, English) was a core model for the project. Additionally, the digital approaches used by other large-scale digital projects, including the grant documentation produced for projects funded by the National Endowment for the Humanities’ Office of Digital Humanities. The larger conversation taking place in forums like HASTAC, led by individuals like Cathy Davidson, was also a useful resource for developing documentation, procedures, and conventions for the project.  While several scholarly organizations have generated documentation for digital projects, at the time this project was under development, the only documentation available for large-scale digital dissertation projects came from George Mason University’s History and Art History Department.  
The long-term preservation plan for the project focuses on describing, preserving, and providing access to discrete components of the dissertation. The core components of the dissertation include the following:
•	Text-based narrative content (what constitutes the bulk of this PDF document)
•	The licensed dataset acquired from Baseball Reference and Hidden Game Sports
•	The harvested data acquired from primary source documents and Twitter
•	The static images and figures generated by analyzing and visualizing the underlying data
•	The interactive digital visualizations based on the underlying data
o	The data that drives those visualizations
Each of those components has a unique and distinct preservation plan, based on the terms of data access, flexibility of available platforms, and institutional resources available for ongoing digital project support.  
The licensed dataset is preserved in the University of Iowa’s “dark archive,” the version of the institutional repository that is not publicly accessible and does not interact with any public-facing systems. The licensed dataset is linked to the public-facing project via a DOI unique identifier. That identifier does not provide access to the full dataset but does provide a description and overview of the archived object. The licensed dataset in its full original form is not publicly-available, and those terms of access will not change, barring a renegotiated licensing agreement that provides for a different level of access. Sample data structures and relevant field descriptions for the licensed dataset are included in this technical whitepaper.
The additional data harvested from primary source documents (information on Collective Bargaining Agreements and members of the Player Relations Committee) does not include any licensing restrictions. That dataset is included in full in the ProQuest deposit and Iowa Research Online deposit as a supplementary data file. That dataset includes a descriptive metadata file that follows Open Data conventions. This dataset is licensed under a Creative Commons 4.0 license. This dataset is linked to the public-facing project via a DOI unique identifier, which provides a description and overview for and access to the archived object.
The data harvested from Twitter is covered by Twitter’s terms of service and API restrictions, which do not allow full Tweet datasets to be published or shared in their originally-harvested form. A “dehydrated” list of unique Tweet IDs is included in full in the ProQuest deposit and Iowa Research Online deposit as a supplementary data file. That list of unique Tweet IDs is also accompanied by a list of the specific Twitter accounts scraped during the data collection period. Both the dehydrated Tweet IDs and Twitter account information are also publicly available via a GitHub repository. That dataset includes a descriptive metadata file that follows Open Data conventions. 
That list of unique IDs The full Twitter dataset of the “rehydrated” Tweets lives in the University of Iowa Libraries’ “dark archive,” along with the licensed dataset. The Twitter dataset is linked to the public-facing project via a DOI unique identifier. That identifier does not provide access to the full dataset but does provide a description and overview of the archived object. The full Twitter dataset is not publicly-available, barring a change in Twitter’s terms of service and API restrictions. Such a change could also result in the list of unique Tweet IDs no longer being publicly-available. 
The static data visualizations and other two-dimensional representations of the data included in this PDF document are also available as high-resolution images in the ProQuest deposit and Iowa Research Online deposit as a supplementary file. That directory of images and figures also includes a Dublin Core XML document describing the inventory of images and figures. These materials are licensed under a Creative Commons 4.0 license. They are linked to the public-facing project via a DOI unique identifier, which provides a description and overview of and access to the archived objects.
With a few exceptions, the data used to create interactive online visualizations (spatial and non-spatial) is part of the licensed dataset. The CSV, KML, GeoJSON, and ESRI Shapefiles are preserved in the University of Iowa’s “dark archive,” along with the licensed dataset and full Twitter dataset. These materials are linked to the public-facing project via a DOI unique identifier. That identifier does not provide access to any of the underlying data files but does provide a description and overview of the archived materials. These materials also include a Dublin Core XML document describing the inventory of underlying data files and file types.
The various digital components of the project will be collected and featured on an interactive website hosted by the Digital Studio for Scholarship and Publishing for a period of five years. At the end of that five-year period, the interactive data visualization components of the project, as well as the central digital project “home” site will no longer be actively supported by the University of Iowa, the University of Iowa Libraries, and the Digital Studio for Scholarship and Publishing. The University of Iowa Libraries’ partnership with the Internet Archive web crawling service will be used to preserve the site before sunsetting begins. At the end of the sunsetting period, the “home” site will redirect to the Iowa Research Online and ProQuest deposits for the project, as well as any other digital home the author (Katherine Walden) has established for the project at the time of sunsetting. While the static text, images, and some underlying data will remain available as part of the Iowa Research Online and ProQuest deposits, the interactive data visualizations (spatial and non-spatial) will no longer be supported and will no longer be operational in their original linked form.
As part of a larger conversation taking place within digital scholarship and digital humanities, this dissertation works to make visible the technological and human structures necessary to undertake large-scale digital scholarship projects. Rather than relegating the labor of organizing, describing, and transcribing to a monograph’s superfluous acknowledgements section, this dissertation builds on a robust body of literature in library and information science to argue those aspects of any scholarly project—in this case a digital project—are themselves a form of intellectual labor and substantive scholarship. Through framing data curation, analysis, and visualization as scholarly activities, this dissertation takes up a truly interdisciplinary DH approach by engagement with a quantitative dataset to enrich how scholars understand and perceive connections between labor, power, and ideology in U.S. professional baseball.
